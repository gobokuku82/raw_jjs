# ê¸°ìˆ ì  ì»´í¬ë„ŒíŠ¸ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ

## ğŸ“‹ ê°œìš”

ë³¸ ë¬¸ì„œëŠ” ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸ í”„ë¡œì íŠ¸ì˜ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ì½”ë“œ ë ˆë²¨ì—ì„œ ìƒì„¸íˆ ë¶„ì„í•˜ê³ , í•¨ìˆ˜ê°„ í˜¸ì¶œ ê´€ê³„ì™€ ë°ì´í„° íë¦„ì„ ê¸°ìˆ ì ìœ¼ë¡œ ë¬¸ì„œí™”í•œ ë³´ê³ ì„œì…ë‹ˆë‹¤.

**ë¶„ì„ ëŒ€ìƒ**: ëª¨ë“  Python ëª¨ë“ˆ ë° ì£¼ìš” í•¨ìˆ˜
**ë¶„ì„ ê¹Šì´**: í•¨ìˆ˜ ë ˆë²¨ ì˜ì¡´ì„± ë° ë°ì´í„° í”Œë¡œìš°

---

## ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„¸ ë¶„ì„

### 1. legal_workflow.py í•¨ìˆ˜ ë¶„ì„

#### A. create_legal_workflow()
```python
def create_legal_workflow() -> StateGraph:
    workflow = StateGraph(LegalWorkflowState)
    workflow.add_node("retrieval", run_retrieval)
    workflow.add_node("analysis", run_analysis)
    workflow.set_entry_point("retrieval")
    workflow.add_edge("retrieval", "analysis")
    workflow.add_edge("analysis", END)
    return workflow.compile()
```

**ì—­í• **: ê¸°ë³¸ ì„ í˜• ì›Œí¬í”Œë¡œìš° ìƒì„±
**ë…¸ë“œ ì—°ê²°**: retrieval â†’ analysis â†’ END
**ìƒíƒœ ê´€ë¦¬**: LegalWorkflowState ì‚¬ìš©

#### B. run_retrieval(state: LegalWorkflowState)
```python
def run_retrieval(state: LegalWorkflowState) -> Dict[str, Any]:
    # RetrievalState ìƒì„±
    retrieval_state = RetrievalState(
        query=state["query"],
        document_types=None,
        categories=None,
        limit=10,
        # ... ê¸°íƒ€ í•„ë“œ ì´ˆê¸°í™”
    )
    
    # ê²€ìƒ‰ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    retrieval_workflow = create_retrieval_workflow()
    result = retrieval_workflow.invoke(retrieval_state)
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        "retrieved_docs": result.get("final_results", []),
        "current_step": "retrieval_complete",
        "messages": state["messages"] + [...]
    }
```

**ì…ë ¥**: LegalWorkflowState
**ì¶œë ¥**: ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡ê³¼ ìƒíƒœ ì—…ë°ì´íŠ¸
**ì˜ì¡´ì„±**: create_retrieval_workflow()

#### C. run_analysis(state: LegalWorkflowState)
```python
def run_analysis(state: LegalWorkflowState) -> Dict[str, Any]:
    retrieved_docs = state.get("retrieved_docs", [])
    
    # ë¶„ì„ ìƒíƒœ ìƒì„±
    analysis_state = AnalysisState(
        document_content=first_doc.get("full_content", ""),
        document_metadata={"title": first_doc.get("title", "")},
        analysis_type="full",
        llm_provider="openai",
        # ... ê¸°íƒ€ í•„ë“œ ì´ˆê¸°í™”
    )
    
    # ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    analysis_workflow = create_analysis_workflow()
    result = analysis_workflow.invoke(analysis_state)
    
    return {
        "analysis_result": result.get("analysis_result", {}),
        "current_step": "analysis_complete",
        "messages": state["messages"] + [...]
    }
```

**ì…ë ¥**: LegalWorkflowState (ê²€ìƒ‰ ê²°ê³¼ í¬í•¨)
**ì¶œë ¥**: ë¶„ì„ ê²°ê³¼ì™€ ìƒíƒœ ì—…ë°ì´íŠ¸
**ì˜ì¡´ì„±**: create_analysis_workflow()

### 2. ì¡°ê±´ë¶€ ì›Œí¬í”Œë¡œìš° ë¶„ì„

#### create_conditional_workflow()
```python
def create_conditional_workflow() -> StateGraph:
    workflow = StateGraph(LegalWorkflowState)
    workflow.add_node("retrieval", run_retrieval)
    workflow.add_node("analysis", run_analysis)
    workflow.add_node("error_handler", handle_error)
    
    workflow.add_conditional_edges(
        "retrieval",
        should_continue,  # ì¡°ê±´ í•¨ìˆ˜
        {
            "analysis": "analysis",
            "error": "error_handler"
        }
    )
    
    workflow.add_conditional_edges(
        "analysis",
        should_continue,
        {
            END: END,
            "error": "error_handler"
        }
    )
```

**íŠ¹ì§•**: ì˜¤ë¥˜ ì²˜ë¦¬ì™€ ì¡°ê±´ë¶€ ë¶„ê¸° ì§€ì›
**ì¡°ê±´ í•¨ìˆ˜**: should_continue()ë¡œ ë‹¤ìŒ ë‹¨ê³„ ê²°ì •

---

## ğŸ” Retrieval Node ìƒì„¸ ë¶„ì„

### 1. RetrievalNode í´ë˜ìŠ¤ êµ¬ì¡°

```python
class RetrievalNode:
    def __init__(self):
        self.reranker = BGEReranker()
    
    # ì£¼ìš” ë©”ì„œë“œë“¤
    def search_postgres(self, state: RetrievalState) -> RetrievalState
    def search_vector_store(self, state: RetrievalState) -> RetrievalState
    def combine_results(self, state: RetrievalState) -> RetrievalState
    def rerank_results(self, state: RetrievalState) -> RetrievalState
    def finalize_results(self, state: RetrievalState) -> RetrievalState
```

### 2. ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ ë¶„ì„

#### A. search_postgres()
```python
def search_postgres(self, state: RetrievalState) -> RetrievalState:
    # SQLite ê²€ìƒ‰ ì‹¤í–‰
    documents = db_manager.search_documents(
        query=state["query"],
        document_types=state.get("document_types"),
        categories=state.get("categories"),
        limit=state.get("limit", 20)
    )
    
    # ê²°ê³¼ ë³€í™˜
    postgres_results = []
    for doc in documents:
        postgres_results.append({
            "id": str(doc.id),
            "title": doc.title,
            "content": doc.content,
            # ... ê¸°íƒ€ í•„ë“œ
            "score": 1.0,
            "search_type": "postgres"
        })
    
    state["postgres_results"] = postgres_results
    return state
```

**ì™¸ë¶€ ì˜ì¡´ì„±**: `db_manager.search_documents()`
**ë°ì´í„° ë³€í™˜**: LegalDocument ORM â†’ Dict í˜•íƒœ

#### B. search_vector_store()
```python
def search_vector_store(self, state: RetrievalState) -> RetrievalState:
    # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
    vector_results = vector_store.search_documents(
        query=state["query"],
        n_results=state.get("limit", 20)
    )
    
    # ê²€ìƒ‰ íƒ€ì… ì¶”ê°€
    for result in vector_results:
        result["search_type"] = "vector"
    
    state["vector_results"] = vector_results
    return state
```

**ì™¸ë¶€ ì˜ì¡´ì„±**: `vector_store.search_documents()`
**íŠ¹ì§•**: ì˜ë¯¸ì  ìœ ì‚¬ë„ ê¸°ë°˜ ê²€ìƒ‰

#### C. combine_results()
```python
def combine_results(self, state: RetrievalState) -> RetrievalState:
    postgres_results = state.get("postgres_results", [])
    vector_results = state.get("vector_results", [])
    
    # ë¬¸ì„œ ID ê¸°ë°˜ ë³‘í•©
    combined_map = {}
    
    # PostgreSQL ê²°ê³¼ ì¶”ê°€
    for result in postgres_results:
        doc_id = result["id"]
        combined_map[doc_id] = result
    
    # ë²¡í„° ê²°ê³¼ ë³‘í•©
    for result in vector_results:
        doc_id = result["id"]
        if doc_id in combined_map:
            # ìŠ¤ì½”ì–´ í‰ê·  ê³„ì‚°
            combined_map[doc_id]["combined_score"] = (
                combined_map[doc_id]["score"] + result["score"]
            ) / 2
            combined_map[doc_id]["search_type"] = "hybrid"
        else:
            result["combined_score"] = result["score"]
            combined_map[doc_id] = result
    
    # ìŠ¤ì½”ì–´ ê¸°ì¤€ ì •ë ¬
    hybrid_results = list(combined_map.values())
    hybrid_results.sort(key=lambda x: x.get("combined_score", 0), reverse=True)
    
    state["hybrid_results"] = hybrid_results[:state.get("limit", 10)]
    return state
```

**ì•Œê³ ë¦¬ì¦˜**: ë¬¸ì„œ ID ê¸°ë°˜ ì¤‘ë³µ ì œê±° ë° ìŠ¤ì½”ì–´ ë³‘í•©
**ì •ë ¬ ê¸°ì¤€**: combined_score ë‚´ë¦¼ì°¨ìˆœ

#### D. rerank_results()
```python
def rerank_results(self, state: RetrievalState) -> RetrievalState:
    hybrid_results = state.get("hybrid_results", [])
    
    if not self.reranker.is_available():
        state["reranked_results"] = hybrid_results
        return state
    
    # BGE ë¦¬ë­ì»¤ ì‚¬ìš©
    reranked = self.reranker.rerank_with_metadata(
        query=state["query"],
        documents=hybrid_results,
        content_key="content",
        top_k=state.get("limit", 10)
    )
    
    state["reranked_results"] = reranked
    return state
```

**ì˜ì¡´ì„±**: BGEReranker í´ë˜ìŠ¤
**Fallback**: ë¦¬ë­ì»¤ ë¯¸ì‚¬ìš© ê°€ëŠ¥ì‹œ ì›ë³¸ ê²°ê³¼ ì‚¬ìš©

### 3. create_retrieval_workflow()
```python
def create_retrieval_workflow() -> StateGraph:
    retrieval_node = RetrievalNode()
    
    workflow = StateGraph(RetrievalState)
    workflow.add_node("search_postgres", retrieval_node.search_postgres)
    workflow.add_node("search_vector", retrieval_node.search_vector_store)
    workflow.add_node("combine", retrieval_node.combine_results)
    workflow.add_node("rerank", retrieval_node.rerank_results)
    workflow.add_node("finalize", retrieval_node.finalize_results)
    
    # ì‹œì‘ì  ì„¤ì •
    workflow.set_entry_point("search_postgres")
    
    # ë³‘ë ¬ ê²€ìƒ‰ í›„ ìˆœì°¨ ì²˜ë¦¬
    workflow.add_edge("search_postgres", "search_vector")
    workflow.add_edge("search_vector", "combine")
    workflow.add_edge("combine", "rerank")
    workflow.add_edge("rerank", "finalize")
    workflow.add_edge("finalize", END)
    
    return workflow.compile()
```

**ë…¸ë“œ íë¦„**: search_postgres â†’ search_vector â†’ combine â†’ rerank â†’ finalize
**ë³‘ë ¬ ì²˜ë¦¬**: PostgreSQLê³¼ ë²¡í„° ê²€ìƒ‰ì´ ìˆœì°¨ì ìœ¼ë¡œ ì‹¤í–‰ (ë³‘ë ¬ ìµœì í™” ê°€ëŠ¥)

---

## ğŸ§  Analysis Node ìƒì„¸ ë¶„ì„

### 1. AnalysisNode í´ë˜ìŠ¤ êµ¬ì¡°

```python
class AnalysisNode:
    def __init__(self):
        self.openai_client = OpenAIClient()
        self.clova_client = ClovaClient()
    
    def _get_llm_client(self, provider: str):
        if provider.lower() == "clova" and self.clova_client.available:
            return self.clova_client
        else:
            return self.openai_client
```

### 2. ë¶„ì„ í•¨ìˆ˜ë“¤

#### A. extract_summary()
```python
def extract_summary(self, state: AnalysisState) -> AnalysisState:
    if state.get("analysis_type") not in ["summary", "full"]:
        return state
    
    llm_client = self._get_llm_client(state.get("llm_provider", "openai"))
    
    summary = llm_client.summarize_text(
        text=state["document_content"],
        summary_type="detailed"
    )
    
    state["summary"] = summary
    return state
```

**ì¡°ê±´ë¶€ ì‹¤í–‰**: analysis_typeì— ë”°ë¼ ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
**LLM ì„ íƒ**: ì„¤ì •ì— ë”°ë¥¸ ë™ì  í´ë¼ì´ì–¸íŠ¸ ì„ íƒ

#### B. identify_legal_issues()
```python
def identify_legal_issues(self, state: AnalysisState) -> AnalysisState:
    llm_client = self._get_llm_client(state.get("llm_provider", "openai"))
    
    system_prompt = """ë‹¹ì‹ ì€ ë²•ë¥  ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì£¼ì–´ì§„ ë¬¸ì„œì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ë²•ì  ìŸì ë“¤ì„ ì‹ë³„í•´ì£¼ì„¸ìš”:
    1. ì£¼ìš” ë²•ì  ìœ„í—˜ìš”ì†Œ
    2. ê·œì • ìœ„ë°˜ ê°€ëŠ¥ì„±
    3. ê³„ì•½ìƒ ë¶„ìŸ ìš”ì†Œ
    4. ê¶Œë¦¬ ë° ì˜ë¬´ ê´€ê³„
    5. ë²•ì  ì ˆì°¨ìƒ ì£¼ì˜ì‚¬í•­"""
    
    messages = [{"role": "user", "content": f"ë‹¤ìŒ ë¬¸ì„œì˜ ë²•ì  ìŸì ì„ ë¶„ì„í•´ì£¼ì„¸ìš”:\n\n{state['document_content']}"}]
    
    response = llm_client.chat_completion(messages, system_prompt=system_prompt)
    
    # ì‘ë‹µ íŒŒì‹±
    legal_issues = []
    for line in response.split('\n'):
        line = line.strip()
        if line and (line[0].isdigit() or line.startswith('-') or line.startswith('â€¢')):
            clean_issue = line.lstrip('0123456789.-â€¢ ').strip()
            if clean_issue:
                legal_issues.append(clean_issue)
    
    state["legal_issues"] = legal_issues if legal_issues else [response]
    return state
```

**í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§**: ë²•ë¥  ë„ë©”ì¸ íŠ¹í™” ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
**ì‘ë‹µ íŒŒì‹±**: êµ¬ì¡°í™”ëœ í…ìŠ¤íŠ¸ì—ì„œ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

#### C. compile_analysis()
```python
def compile_analysis(self, state: AnalysisState) -> AnalysisState:
    analysis_result = {
        "document_metadata": state.get("document_metadata", {}),
        "analysis_type": state.get("analysis_type", ""),
        "summary": state.get("summary"),
        "key_points": state.get("key_points", []),
        "legal_issues": state.get("legal_issues", []),
        "entities": state.get("entities", {}),
        "recommendations": state.get("recommendations", []),
        "risk_assessment": state.get("risk_assessment"),
        "timestamp": datetime.now().isoformat()
    }
    
    state["analysis_result"] = analysis_result
    return state
```

**ì—­í• **: ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ êµ¬ì¡°ì²´ë¡œ í†µí•©

### 3. create_analysis_workflow()
```python
def create_analysis_workflow() -> StateGraph:
    analysis_node = AnalysisNode()
    
    workflow = StateGraph(AnalysisState)
    workflow.add_node("extract_summary", analysis_node.extract_summary)
    workflow.add_node("extract_key_points", analysis_node.extract_key_points)
    workflow.add_node("identify_legal_issues", analysis_node.identify_legal_issues)
    workflow.add_node("extract_entities", analysis_node.extract_entities)
    workflow.add_node("generate_recommendations", analysis_node.generate_recommendations)
    workflow.add_node("assess_risk", analysis_node.assess_risk)
    workflow.add_node("compile_analysis", analysis_node.compile_analysis)
    
    workflow.set_entry_point("extract_summary")
    
    # ìˆœì°¨ ì‹¤í–‰
    workflow.add_edge("extract_summary", "extract_key_points")
    workflow.add_edge("extract_key_points", "identify_legal_issues")
    workflow.add_edge("identify_legal_issues", "extract_entities")
    workflow.add_edge("extract_entities", "generate_recommendations")
    workflow.add_edge("generate_recommendations", "assess_risk")
    workflow.add_edge("assess_risk", "compile_analysis")
    workflow.add_edge("compile_analysis", END)
    
    return workflow.compile()
```

**ì‹¤í–‰ ìˆœì„œ**: ìš”ì•½ â†’ í•µì‹¬ì‚¬í•­ â†’ ë²•ì ìŸì  â†’ ê°œì²´ëª… â†’ ê¶Œê³ ì‚¬í•­ â†’ ìœ„í—˜í‰ê°€ â†’ í†µí•©
**ë³‘ë ¬í™” ê°€ëŠ¥ì„±**: ì¼ë¶€ ë‹¨ê³„ëŠ” ë…ë¦½ì ìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥

---

## ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ë ˆì´ì–´ ë¶„ì„

### 1. SQLiteManager í´ë˜ìŠ¤

#### A. ê²€ìƒ‰ í•¨ìˆ˜ ë¶„ì„
```python
def search_documents(
    self, 
    query: str, 
    document_types: Optional[List[str]] = None,
    categories: Optional[List[str]] = None,
    limit: int = 10,
    offset: int = 0
) -> List[LegalDocument]:
    
    with self.get_session() as session:
        db_query = session.query(LegalDocumentORM)
        
        # í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì¡°ê±´
        if query:
            search_condition = text(
                "(title LIKE :query OR content LIKE :query)"
            )
            db_query = db_query.filter(search_condition).params(query=f"%{query}%")
        
        # í•„í„° ì ìš©
        if document_types:
            db_query = db_query.filter(LegalDocumentORM.document_type.in_(document_types))
        
        if categories:
            db_query = db_query.filter(LegalDocumentORM.category.in_(categories))
        
        # í˜ì´ì§€ë„¤ì´ì…˜
        documents = db_query.offset(offset).limit(limit).all()
        
        return [LegalDocument.from_orm(doc) for doc in documents]
```

**ê²€ìƒ‰ ë°©ì‹**: LIKE ì—°ì‚°ìë¥¼ í†µí•œ ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­
**í•„í„°ë§**: document_type, categoryë³„ í•„í„°ë§ ì§€ì›
**ORM ë³€í™˜**: LegalDocumentORM â†’ LegalDocument Pydantic ëª¨ë¸

### 2. VectorStoreManager í´ë˜ìŠ¤

#### A. ë¬¸ì„œ ì¶”ê°€ í•¨ìˆ˜
```python
def add_document(
    self, 
    document_id: str, 
    content: str, 
    metadata: Optional[Dict[str, Any]] = None
) -> bool:
    
    # ì„ë² ë”© ìƒì„±
    embedding = self.embeddings.embed_text(content)
    
    # ChromaDB ì»¬ë ‰ì…˜ì— ì¶”ê°€
    self.collection.add(
        embeddings=[embedding],
        documents=[content],
        metadatas=[metadata or {}],
        ids=[document_id]
    )
    
    return True
```

**ì˜ì¡´ì„±**: KUREEmbeddings.embed_text()
**ì €ì¥ í˜•íƒœ**: ì„ë² ë”©, ì›ë³¸ í…ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° í•¨ê»˜ ì €ì¥

#### B. ë²¡í„° ê²€ìƒ‰ í•¨ìˆ˜
```python
def search_documents(
    self, 
    query: str, 
    n_results: int = 10,
    where: Optional[Dict[str, Any]] = None
) -> List[Dict[str, Any]]:
    
    # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = self.embeddings.embed_text(query)
    
    # ChromaDB ê²€ìƒ‰
    results = self.collection.query(
        query_embeddings=[query_embedding],
        n_results=n_results,
        where=where,
        include=["documents", "metadatas", "distances"]
    )
    
    # ê²°ê³¼ í¬ë§·íŒ…
    formatted_results = []
    for i in range(len(results["ids"][0])):
        formatted_results.append({
            "id": results["ids"][0][i],
            "document": results["documents"][0][i],
            "metadata": results["metadatas"][0][i],
            "score": 1 - results["distances"][0][i]  # ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ë¡œ ë³€í™˜
        })
    
    return formatted_results
```

**ìœ ì‚¬ë„ ê³„ì‚°**: ì½”ì‚¬ì¸ ê±°ë¦¬ë¥¼ ìœ ì‚¬ë„ ì ìˆ˜ë¡œ ë³€í™˜
**ë©”íƒ€ë°ì´í„° í•„í„°ë§**: where ì¡°ê±´ì„ í†µí•œ í•„í„°ë§ ì§€ì›

---

## ğŸ§  LLM í´ë¼ì´ì–¸íŠ¸ ë¶„ì„

### 1. OpenAIClient í´ë˜ìŠ¤

#### A. ê³µí†µ ì±„íŒ… ì™„ë£Œ í•¨ìˆ˜
```python
def chat_completion(
    self, 
    messages: List[Dict[str, str]], 
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    system_prompt: Optional[str] = None
) -> str:
    
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
    if system_prompt:
        messages = [{"role": "system", "content": system_prompt}] + messages
    
    response = self.client.chat.completions.create(
        model=self.model,
        messages=messages,
        temperature=temperature or self.temperature,
        max_tokens=max_tokens or self.max_tokens
    )
    
    return response.choices[0].message.content
```

**ëª¨ë¸**: GPT-4o
**ë§¤ê°œë³€ìˆ˜**: temperature=0.1 (ë²•ë¥  ë¶„ì•¼ ì¼ê´€ì„±ì„ ìœ„í•œ ë‚®ì€ ì˜¨ë„)

#### B. íŠ¹í™” ë¶„ì„ í•¨ìˆ˜ë“¤
```python
def analyze_legal_document(self, document_content: str, analysis_type: str = "summary") -> str:
    system_prompt = """ë‹¹ì‹ ì€ ì „ë¬¸ ë²•ë¥  AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 
    ë²•ë¥  ë¬¸ì„œë¥¼ ì •í™•í•˜ê³  ì²´ê³„ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ì •ë³´ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”:
    1. ë¬¸ì„œì˜ í•µì‹¬ ìš”ì•½
    2. ì£¼ìš” ë²•ì  ìŸì 
    3. ê´€ë ¨ ë²•ë ¹ ë° íŒë¡€
    4. ì£¼ì˜ì‚¬í•­ ë° ê¶Œê³ ì‚¬í•­
    
    ë¶„ì„ì€ ê°ê´€ì ì´ê³  ì •í™•í•´ì•¼ í•˜ë©°, ë²•ì  ê·¼ê±°ë¥¼ ëª…í™•íˆ ì œì‹œí•´ì£¼ì„¸ìš”."""
    
    user_message = f"""ë‹¤ìŒ ë²•ë¥  ë¬¸ì„œë¥¼ ë¶„ì„í•´ì£¼ì„¸ìš”:

ë¬¸ì„œ ë‚´ìš©:
{document_content}

ë¶„ì„ ìœ í˜•: {analysis_type}"""
    
    messages = [{"role": "user", "content": user_message}]
    
    return self.chat_completion(messages, system_prompt=system_prompt)
```

**ë„ë©”ì¸ íŠ¹í™”**: ë²•ë¥  ë¬¸ì„œ ë¶„ì„ì„ ìœ„í•œ íŠ¹ë³„í•œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸

### 2. ClovaClient í´ë˜ìŠ¤

#### A. API ìš”ì²­ í•¨ìˆ˜
```python
def _make_request(self, payload: Dict[str, Any]) -> Dict[str, Any]:
    headers = {
        "X-NCP-CLOVASTUDIO-API-KEY": self.api_key,
        "X-NCP-APIGW-API-KEY": self.apigw_api_key,
        "Content-Type": "application/json",
        "Accept": "text/event-stream"
    }
    
    response = requests.post(
        self.base_url,
        headers=headers,
        json=payload,
        timeout=30
    )
    
    return response.json()
```

**ì¸ì¦**: ì´ì¤‘ API í‚¤ (CLOVASTUDIO + APIGW)
**ì—”ë“œí¬ì¸íŠ¸**: HyperClova-X ì „ìš© API

---

## ğŸ¨ Streamlit App ë¶„ì„

### 1. ë©”ì¸ ì•± êµ¬ì¡°

#### A. íƒ­ ê¸°ë°˜ ë„¤ë¹„ê²Œì´ì…˜
```python
def main():
    init_session_state()
    display_header()
    settings_dict = display_sidebar()
    
    # íƒ­ ìƒì„±
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ” ë¬¸ì„œ ê²€ìƒ‰", 
        "ğŸ“Š ë¬¸ì„œ ë¶„ì„", 
        "ğŸ’¬ ë²•ë¥  Q&A", 
        "ğŸ”§ ì‹œìŠ¤í…œ í˜„í™©"
    ])
    
    with tab1:
        document_search_tab()
    with tab2:
        document_analysis_tab()
    with tab3:
        legal_qa_tab()
    with tab4:
        system_status_tab()
```

#### B. ë¹„ë™ê¸° ê²€ìƒ‰ í•¨ìˆ˜
```python
async def search_documents(query: str, settings_dict: dict):
    retrieval_workflow = create_retrieval_workflow()
    
    initial_state: RetrievalState = {
        "query": query,
        "document_types": None,
        "categories": None,
        "limit": settings_dict["max_results"],
        # ... ê¸°íƒ€ ì´ˆê¸°ê°’
    }
    
    # ì§„í–‰ ìƒí™© í‘œì‹œ
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
    result = await retrieval_workflow.ainvoke(initial_state)
    
    return result.get("final_results", [])
```

**UI í”¼ë“œë°±**: ì§„í–‰ ìƒí™© ë°”ì™€ ìƒíƒœ í…ìŠ¤íŠ¸
**ë¹„ë™ê¸° ì²˜ë¦¬**: LangGraph ì›Œí¬í”Œë¡œìš°ì˜ ë¹„ë™ê¸° ì‹¤í–‰

---

## ğŸ“Š ì„±ëŠ¥ ë° ìµœì í™” ê³ ë ¤ì‚¬í•­

### 1. ë³‘ëª© ì§€ì  ë¶„ì„

#### A. LLM API í˜¸ì¶œ
- **ì§€ì—°ì‹œê°„**: OpenAI/Clova API ì‘ë‹µ ì‹œê°„
- **ìµœì í™” ë°©ì•ˆ**: ì‘ë‹µ ìºì‹±, ë³‘ë ¬ ì²˜ë¦¬

#### B. ì„ë² ë”© ìƒì„±
- **KURE ëª¨ë¸ ë¡œë”©**: ì´ˆê¸° ë¡œë”© ì‹œê°„
- **ìµœì í™” ë°©ì•ˆ**: ëª¨ë¸ ì‚¬ì „ ë¡œë”©, GPU í™œìš©

#### C. ë²¡í„° ê²€ìƒ‰
- **ChromaDB ì¸ë±ìŠ¤**: ëŒ€ìš©ëŸ‰ ë°ì´í„°ì—ì„œ ê²€ìƒ‰ ì†ë„
- **ìµœì í™” ë°©ì•ˆ**: ì¸ë±ìŠ¤ íŠœë‹, ë¶„ì‚° ì €ì¥

### 2. ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰

#### A. ë¬¸ì„œ ë¡œë”©
```python
# ì „ì²´ ë¬¸ì„œ ë‚´ìš©ì„ ë©”ëª¨ë¦¬ì— ë¡œë”©
document_content = first_doc.get("full_content", "")
```

**ê°œì„  ë°©ì•ˆ**: ì²­í‚¹ ê¸°ë°˜ ì²˜ë¦¬, ìŠ¤íŠ¸ë¦¬ë° ë¶„ì„

#### B. ì„ë² ë”© ë°°ì¹˜ ì²˜ë¦¬
```python
embeddings = self.model.encode(processed_texts, convert_to_tensor=False, batch_size=32)
```

**ìµœì í™”**: ë°°ì¹˜ í¬ê¸° ì¡°ì •, ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§

---

## ğŸ”— ì˜ì¡´ì„± ê·¸ë˜í”„

### 1. ëª¨ë“ˆ ê°„ ì˜ì¡´ì„±

```
app/main.py
â”œâ”€â”€ workflows/graphs/legal_workflow.py
â”‚   â”œâ”€â”€ workflows/nodes/retrieval.py
â”‚   â”‚   â”œâ”€â”€ core/database/sqlite.py
â”‚   â”‚   â”œâ”€â”€ core/database/vector_store.py
â”‚   â”‚   â”‚   â””â”€â”€ core/embeddings/kure_embeddings.py
â”‚   â”‚   â””â”€â”€ core/embeddings/reranker.py
â”‚   â””â”€â”€ workflows/nodes/analysis.py
â”‚       â”œâ”€â”€ core/llm/openai_client.py
â”‚       â””â”€â”€ core/llm/clova_client.py
â””â”€â”€ core/simple_config.py
```

### 2. í•¨ìˆ˜ í˜¸ì¶œ ì²´ì¸

```
ì‚¬ìš©ì ì…ë ¥
â”œâ”€â”€ main.py:search_documents()
â”‚   â””â”€â”€ legal_workflow.py:create_legal_workflow().invoke()
â”‚       â”œâ”€â”€ run_retrieval()
â”‚       â”‚   â””â”€â”€ create_retrieval_workflow().invoke()
â”‚       â”‚       â”œâ”€â”€ RetrievalNode.search_postgres()
â”‚       â”‚       â”‚   â””â”€â”€ db_manager.search_documents()
â”‚       â”‚       â”œâ”€â”€ RetrievalNode.search_vector_store()
â”‚       â”‚       â”‚   â””â”€â”€ vector_store.search_documents()
â”‚       â”‚       â”‚       â””â”€â”€ embeddings.embed_text()
â”‚       â”‚       â”œâ”€â”€ RetrievalNode.combine_results()
â”‚       â”‚       â”œâ”€â”€ RetrievalNode.rerank_results()
â”‚       â”‚       â””â”€â”€ RetrievalNode.finalize_results()
â”‚       â””â”€â”€ run_analysis()
â”‚           â””â”€â”€ create_analysis_workflow().invoke()
â”‚               â”œâ”€â”€ AnalysisNode.extract_summary()
â”‚               â”‚   â””â”€â”€ llm_client.summarize_text()
â”‚               â”œâ”€â”€ AnalysisNode.extract_key_points()
â”‚               â”œâ”€â”€ AnalysisNode.identify_legal_issues()
â”‚               â”œâ”€â”€ AnalysisNode.extract_entities()
â”‚               â”œâ”€â”€ AnalysisNode.generate_recommendations()
â”‚               â”œâ”€â”€ AnalysisNode.assess_risk()
â”‚               â””â”€â”€ AnalysisNode.compile_analysis()
â””â”€â”€ main.py:display_search_results()
```

---

## ğŸ“‹ ê²°ë¡ 

ë³¸ ì‹œìŠ¤í…œì€ LangGraphë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ í•œ ëª…í™•í•œ ëª¨ë“ˆí™” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë©°, ê° ì»´í¬ë„ŒíŠ¸ê°€ ëª…í™•í•œ ì±…ì„ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ ë ˆë²¨ì—ì„œì˜ ì—°ê²° ê´€ê³„ê°€ ì˜ ì •ì˜ë˜ì–´ ìˆì–´ ìœ ì§€ë³´ìˆ˜ì™€ í™•ì¥ì´ ìš©ì´í•œ êµ¬ì¡°ì…ë‹ˆë‹¤.

ì£¼ìš” ê°œì„  í¬ì¸íŠ¸ëŠ” ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”ì™€ ìºì‹± ì‹œìŠ¤í…œ ë„ì…ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒì´ë©°, í˜„ì¬ì˜ ëª¨ë“ˆí™”ëœ êµ¬ì¡°ëŠ” ì´ëŸ¬í•œ ê°œì„ ì‚¬í•­ë“¤ì„ ì ìš©í•˜ê¸°ì— ì í•©í•œ ê¸°ë°˜ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤.

---

**ë³´ê³ ì„œ ì‘ì„±ì**: AI Assistant  
**ë¶„ì„ì¼**: 2024ë…„ 12ì›” 26ì¼ 